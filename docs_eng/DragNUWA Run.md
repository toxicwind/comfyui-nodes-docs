# Documentation
- Class name: DragNUWARun
- Category: DragNUWA
- Output node: False
- Repo Ref: https://github.com/chaojie/ComfyUI-DragNUWA.git

DragNUWARun nodes are designed to perform advanced image processing and reasoning tasks, using a predefined model to analyse and manipulate image data on the basis of tracking points and other input parameters. It is designed to enhance image sequences by applying motion estimation and light flow techniques that help generate high-quality visual output.

# Input types
## Required
- model
    - Model parameters are essential because it defines the basic architecture and pre-training weights used for image processing tasks. It determines the ability of nodes to interpret and convert images according to the models and characteristics they learn.
    - Comfy dtype: DragNUWA
    - Python dtype: torch.nn.Module
- image
    - Image input is essential for node execution of its main function, image analysis and reasoning. It is the raw data that the model will process and its contents directly affect the quality and accuracy of node output.
    - Comfy dtype: IMAGE
    - Python dtype: PIL.Image or numpy.ndarray
- tracking_points
    - Tracking points are essential for nodes because they provide the spatial information needed to understand and predict motion in the image sequence. They are used to guide the conversion and enhancement process and to ensure that output is consistent with the desired dynamics of the movement.
    - Comfy dtype: STRING
    - Python dtype: str
- inference_batch_size
    - Logical batch size parameters optimize the processing efficiency of nodes by determining how many images are processed simultaneously. It affects the trade-off between the calculation of resources and processing speed, thus affecting the overall performance of the nodes.
    - Comfy dtype: INT
    - Python dtype: int
- motion_bucket_id
    - The motor drum ID is a key parameter for classifying the type of motion in order for the model to apply a specific conversion. It helps to achieve the desired motor effect in the output by selecting the appropriate campaign configuration file from the predefined collection.
    - Comfy dtype: INT
    - Python dtype: int
- use_optical_flow
    - The application of light flow techniques to motion estimation is enabled or disabled using light flow parameters. It is important for nodes to accurately predict and simulate complex movements in the image sequence.
    - Comfy dtype: BOOLEAN
    - Python dtype: bool
- directory
    - Catalogue parameters specify the location where to store or retrieve the optical stream data. This is essential for node access to the necessary campaign information, which is essential for accurate image-processing tasks.
    - Comfy dtype: STRING
    - Python dtype: str

# Output types
- output
    - Output parameters represent the final processed image or image sequences generated by node operations. It covers node conversion and enhances the ability to enter data, reflecting the validity of application models and algorithms.
    - Comfy dtype: IMAGE
    - Python dtype: PIL.Image or numpy.ndarray

# Usage tips
- Infra type: GPU

# Source code
```
class DragNUWARun:

    @classmethod
    def INPUT_TYPES(cls):
        return {'required': {'model': ('DragNUWA',), 'image': ('IMAGE',), 'tracking_points': ('STRING', {'multiline': True, 'default': '[[[25,25],[128,128]]]'}), 'inference_batch_size': ('INT', {'default': 1, 'min': 1, 'max': 1}), 'motion_bucket_id': ('INT', {'default': 4, 'min': 1, 'max': 100}), 'use_optical_flow': ('BOOLEAN', {'default': False}), 'directory': ('STRING', {'default': 'X://path/to/optical_flow', 'vhs_path_extensions': []})}}
    RETURN_TYPES = ('IMAGE',)
    FUNCTION = 'run_inference'
    CATEGORY = 'DragNUWA'

    def run_inference(self, model, image, tracking_points, inference_batch_size, motion_bucket_id, use_optical_flow, directory):
        image = 255.0 * image[0].cpu().numpy()
        image_pil = Image.fromarray(np.clip(image, 0, 255).astype(np.uint8))
        (raw_w, raw_h) = image_pil.size
        resize_ratio = max(model.width / raw_w, model.height / raw_h)
        image_pil = image_pil.resize((int(raw_w * resize_ratio), int(raw_h * resize_ratio)), Image.BILINEAR)
        image_pil = transforms.CenterCrop((model.height, model.width))(image_pil.convert('RGB'))
        tracking_points = json.loads(tracking_points)
        return model.run_2(image_pil, tracking_points, inference_batch_size, motion_bucket_id, use_optical_flow, directory)
```