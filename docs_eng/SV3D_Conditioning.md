# Documentation
- Class name: SV3D_Conditioning
- Category: conditioning/3d_models
- Output node: False
- Repo Ref: https://github.com/comfyanonymous/ComfyUI

The SV3D_Conditioning node is designed to process and encode visual and spatial information to generate 3D condition data. It accepts various inputs, such as images, visual models and spatial parameters, to generate a condition signal that can be used to generate a 3D model. The node plays a key role in aligning the 3D model generated with the input visual and spatial context.

# Input types
## Required
- clip_vision
    - The clip_vision parameter is essential for the node because it provides a visual model to encode the initial image. This input directly affects the quality and accuracy of the encoded image, which is essential for the subsequent 3D model generation process.
    - Comfy dtype: CLIP_VISION
    - Python dtype: torch.nn.Module
- init_image
    - The init_image parameter is the key input of the node that will be used to adjust the initial image of the 3D model. It is essential to define the visual aspects of the output of the 3D model and must be carefully selected to ensure the desired result.
    - Comfy dtype: IMAGE
    - Python dtype: PIL.Image.Image
- vae
    - The vae parameter represents the variable-based encoder (VAE) used in the node to encode pixel data to potential space. This conversion is essential to create a rich representation of characteristics that can be used to regulate 3D models.
    - Comfy dtype: VAE
    - Python dtype: torch.nn.Module
- width
    - The width parameter specifies the width needed to magnify the image. It is a key factor in determining the resolution and level of detail of the final 3D model, and higher values usually lead to more detailed output.
    - Comfy dtype: INT
    - Python dtype: int
- height
    - The height parameter sets the height needed to magnify the image. Together with width, it determines the overall size of the image to be encoded and then influences the details and the authenticity of the 3D model.
    - Comfy dtype: INT
    - Python dtype: int
## Optional
- video_frames
    - The video_frames parameter determines the number of frames to be generated for video output. It is an optional input that can be adjusted to the specific requirements of the 3D model task.
    - Comfy dtype: INT
    - Python dtype: int
- elevation
    - The elevation parameter provides a vertical angle for viewing the 3D model. It is an optional input that allows the view angle to be adjusted to achieve the desired perspective in the final output.
    - Comfy dtype: FLOAT
    - Python dtype: float

# Output types
- positive
    - The postive output consists of encoded images embedded and spatial parameters, which are the positive-regime signals generated by the 3D model. This output is very important because it directly helps to shape the final 3D model based on the input image and the spatial context.
    - Comfy dtype: CONDITIONING
    - Python dtype: List[Tuple[torch.Tensor, Dict[str, Any]]}
- negative
    - Negative output contains zero-value images embedded and spatial parameters that serve as a negative-direction adjustment signal. This output is used to provide a comparison with a positive signal and helps refine the 3D modeling process.
    - Comfy dtype: CONDITIONING
    - Python dtype: List[Tuple[torch.Tensor, Dict[str, Any]]
- latent
    - The latent output represents the potential spatial expression for magnifying the image. It is a key component of the 3D model because it contains visual features extracted from the input image.
    - Comfy dtype: LATENT
    - Python dtype: Dict[str, torch.Tensor]

# Usage tips
- Infra type: GPU

# Source code
```
class SV3D_Conditioning:

    @classmethod
    def INPUT_TYPES(s):
        return {'required': {'clip_vision': ('CLIP_VISION',), 'init_image': ('IMAGE',), 'vae': ('VAE',), 'width': ('INT', {'default': 576, 'min': 16, 'max': nodes.MAX_RESOLUTION, 'step': 8}), 'height': ('INT', {'default': 576, 'min': 16, 'max': nodes.MAX_RESOLUTION, 'step': 8}), 'video_frames': ('INT', {'default': 21, 'min': 1, 'max': 4096}), 'elevation': ('FLOAT', {'default': 0.0, 'min': -90.0, 'max': 90.0, 'step': 0.1, 'round': False})}}
    RETURN_TYPES = ('CONDITIONING', 'CONDITIONING', 'LATENT')
    RETURN_NAMES = ('positive', 'negative', 'latent')
    FUNCTION = 'encode'
    CATEGORY = 'conditioning/3d_models'

    def encode(self, clip_vision, init_image, vae, width, height, video_frames, elevation):
        output = clip_vision.encode_image(init_image)
        pooled = output.image_embeds.unsqueeze(0)
        pixels = comfy.utils.common_upscale(init_image.movedim(-1, 1), width, height, 'bilinear', 'center').movedim(1, -1)
        encode_pixels = pixels[:, :, :, :3]
        t = vae.encode(encode_pixels)
        azimuth = 0
        azimuth_increment = 360 / (max(video_frames, 2) - 1)
        elevations = []
        azimuths = []
        for i in range(video_frames):
            elevations.append(elevation)
            azimuths.append(azimuth)
            azimuth += azimuth_increment
        positive = [[pooled, {'concat_latent_image': t, 'elevation': elevations, 'azimuth': azimuths}]]
        negative = [[torch.zeros_like(pooled), {'concat_latent_image': torch.zeros_like(t), 'elevation': elevations, 'azimuth': azimuths}]]
        latent = torch.zeros([video_frames, 4, height // 8, width // 8])
        return (positive, negative, {'samples': latent})
```