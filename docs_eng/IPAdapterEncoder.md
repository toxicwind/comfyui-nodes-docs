# Documentation
- Class name: IPAdapterEncoder
- Category: ipadapter/embeds
- Output node: False
- Repo Ref: https://github.com/cubiq/ComfyUI_IPAdapter_plus.git

IPAdapterEncoder node is designed to process and encode image data using pre-trained CLIPVision models. It uses the power of the CLIPVision architecture to generate embedded captures of semantic information in the image. This node can be embedded with conditions and non-conditions based on the presence and weight parameters of the mask, allowing fine-tuning the impact of the image content on the embedding generated.

# Input types
## Required
- ipadapter
    - The ipadapter parameter is essential for the operation of the node, as it provides a model for encoded image data. It is expected to be a dictionary containing the model information required for the encoding process. The presence of this parameter ensures that the node can access the required model to generate embedding.
    - Comfy dtype: IPADAPTER
    - Python dtype: Dict[str, Any]
- image
    - The image parameter is necessary because it means the input data that the node will process. It is expected to be an image stencil and the node will be encoded as embedded. The quality and format of the image data directly influences the ability of the node to generate meaningful embedding.
    - Comfy dtype: IMAGE
    - Python dtype: torch.Tensor
- weight
    - The weight parameter allows you to adjust the impact of the image content on the nesting that is generated. It is a floating number that can be scaled and embedded to highlight or dilute certain aspects of the image data when it is not equal to 1.
    - Comfy dtype: FLOAT
    - Python dtype: float
## Optional
- mask
    - The optional mask parameters can be used to apply a space mask to image data before encoded. This may be useful for focusing attention on the particular area of the image or excluding unrelated parts of the image from the encoding process.
    - Comfy dtype: MASK
    - Python dtype: torch.Tensor
- clip_vision
    - The clip_vision parameter is an optional model that can be provided to nodes to encode image data. If it is not available, the node will use the model specified in the ipadapter parameter. This allows different CLIPVision models to be used in different encoded tasks.
    - Comfy dtype: CLIP_VISION
    - Python dtype: Any

# Output types
- pos_embed
    - os_embed output is embedded for the conditions generated by the node. These embeddings are influenced by any mask of image data and application and capture semantic information in the image.
    - Comfy dtype: EMBEDS
    - Python dtype: torch.Tensor
- neg_embed
    - Neg_embed output provides non-conditional embedding, which is generated without taking into account the contents of the image. These embeddings can serve as a baseline or a reference point for comparison with conditions embedded.
    - Comfy dtype: EMBEDS
    - Python dtype: torch.Tensor

# Usage tips
- Infra type: GPU

# Source code
```
class IPAdapterEncoder:

    @classmethod
    def INPUT_TYPES(s):
        return {'required': {'ipadapter': ('IPADAPTER',), 'image': ('IMAGE',), 'weight': ('FLOAT', {'default': 1.0, 'min': -1.0, 'max': 3.0, 'step': 0.01})}, 'optional': {'mask': ('MASK',), 'clip_vision': ('CLIP_VISION',)}}
    RETURN_TYPES = ('EMBEDS', 'EMBEDS')
    RETURN_NAMES = ('pos_embed', 'neg_embed')
    FUNCTION = 'encode'
    CATEGORY = 'ipadapter/embeds'

    def encode(self, ipadapter, image, weight, mask=None, clip_vision=None):
        if 'ipadapter' in ipadapter:
            ipadapter_model = ipadapter['ipadapter']['model']
            clip_vision = clip_vision if clip_vision is not None else ipadapter['clipvision']['model']
        else:
            ipadapter_model = ipadapter
            clip_vision = clip_vision
        if clip_vision is None:
            raise Exception('Missing CLIPVision model.')
        is_plus = 'proj.3.weight' in ipadapter_model['image_proj'] or 'latents' in ipadapter_model['image_proj'] or 'perceiver_resampler.proj_in.weight' in ipadapter_model['image_proj']
        if mask is not None and mask.shape[1:3] != torch.Size([224, 224]):
            mask = mask.unsqueeze(1)
            transforms = T.Compose([T.CenterCrop(min(mask.shape[2], mask.shape[3])), T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC, antialias=True)])
            mask = transforms(mask).squeeze(1)
        img_cond_embeds = encode_image_masked(clip_vision, image, mask)
        if is_plus:
            img_cond_embeds = img_cond_embeds.penultimate_hidden_states
            img_uncond_embeds = encode_image_masked(clip_vision, torch.zeros([1, 224, 224, 3])).penultimate_hidden_states
        else:
            img_cond_embeds = img_cond_embeds.image_embeds
            img_uncond_embeds = torch.zeros_like(img_cond_embeds)
        if weight != 1:
            img_cond_embeds = img_cond_embeds * weight
        return (img_cond_embeds, img_uncond_embeds)
```